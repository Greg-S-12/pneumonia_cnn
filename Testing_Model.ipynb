{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "In this notebook the UNSEEN test data will be used to evaluate the final performance and therefore generalisability of the model.\n",
    "<br>A confusion matrix will be generated to show which samples were correctly/incorrectly classified and the overall accuracy and recall of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import data_prep as dp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "from keras.layers import SeparableConv2D, BatchNormalization, Dropout\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment seed for Python\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "seed=101\n",
    "\n",
    "# Set seed for Numpy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all_xrays/test/normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5cc150ec4217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagetype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubfolders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of test examples: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of labels:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Flatiron DS/pneumonia_cnn/data_prep.py\u001b[0m in \u001b[0;36mimage_data_and_labels\u001b[0;34m(imagetype, directory, subfolders, class_labels)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0msubfolder_directory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfolder_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimagetype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# List of images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Flatiron DS/pneumonia_cnn/data_prep.py\u001b[0m in \u001b[0;36mfind_files\u001b[0;34m(file_directory, condition)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_directory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all_xrays/test/normal'"
     ]
    }
   ],
   "source": [
    "imagetype='.jpeg'\n",
    "directory='all_xrays/test/'\n",
    "subfolders=['normal','virus','bacteria']\n",
    "class_labels=[0,1,1]\n",
    "\n",
    "test_data,test_labels=dp.image_data_and_labels(imagetype, directory, subfolders, class_labels)\n",
    "print(\"Total number of test examples: \", test_data.shape)\n",
    "print(\"Total number of labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to an error in this version of Keras we cannot directly load the model weights - there is currently no fix but several workarounds. Inconveniently this requires you know the model architecture. From here you can 'train' the model with 0 epochs (to initialise the weights at some value) from there you can load in the weights and test the model. This is what is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-70893927f2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "model_predict = Sequential()\n",
    "\n",
    "model_predict.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[0])\n",
    "model_predict.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[1])\n",
    "model_predict.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[2])\n",
    "model_predict.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[3])\n",
    "\n",
    "model_predict.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_1'))\n",
    "model_predict.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool1'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_1'))\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool2'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_1'))\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool3'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_1'))\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool4'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_1'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_2'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool5'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv6_1'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv6_2'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool6'))\n",
    "\n",
    "model_predict.add(Flatten(name=\"Flatten\"))\n",
    "model_predict.add(Dense(units=1024,activation=\"relu\", name='Dense1'))\n",
    "model_predict.add(Dense(units=512,activation=\"relu\", name='Dense2'))\n",
    "model_predict.add(Dense(units=2, activation=\"softmax\", name='Result'))\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "opt = Adam(lr=0.0005)\n",
    "checkpoint = ModelCheckpoint(\"baseline.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model_predict.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model_predict.fit(x=train_data,y=train_labels,epochs=0,callbacks=[checkpoint,early],class_weight={0:2.7,1:1})\n",
    "\n",
    "\n",
    "model_predict=load_model(\"Models/2 Class/Best model/Model_4_2class_datagen_OneOf.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict.load_weights('Best model/Model_4_2class_datagen_OneOf.h5')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predicted labels - argmax will select the largest value (the highest probability) and the corresponding label.\n",
    "preds = model_predict.predict(test_data, batch_size=16)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# Original labels \n",
    "orig_test_labels = np.argmax(test_labels, axis=-1)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm  = confusion_matrix(orig_test_labels, preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at Recall, TPR, FPR and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TPR, FPR. Beware of accuracy in imbalanced datasets - we should use ROC curve.\n",
    "\n",
    "tpr = cm[1][1]/(cm[1][0]+cm[1][1])\n",
    "fpr = cm[0][0]/(cm[0][0]+cm[0][1])\n",
    "acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])\n",
    "\n",
    "tpr,fpr,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
