{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "In this notebook, the **unseen** test data will be used to evaluate the final performance and therefore generalisability of the model.\n",
    "<br>A confusion matrix will be generated to show which samples were correctly/incorrectly classified and the overall accuracy and recall of the model.\n",
    "<br><br> It is important to note the metric used to evaluate the final performance of the model. For an imbalanced dataset accuracy can be misleading. For example:\n",
    "<br> If given a credit card dataset of 99% genuine transactions with only 1% fraudulent. If we wish to classify fraudulent cases, we may find our model has a 99% accuracy but (without generating confusion matrix to see) this could mean we correctly identify the genuine cases (99%) but miss all fraudulent cases (1%) resulting a redundant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import data_prep as dp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "from keras.layers import SeparableConv2D, BatchNormalization, Dropout\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment seed for Python\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "seed=101\n",
    "\n",
    "# Set seed for Numpy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagetype = '.jpeg'\n",
    "directory = 'Docs/all_xrays/test/'\n",
    "subfolders = ['normal','virus','bacteria']\n",
    "class_labels = [0,1,1] # Change for binary or multi-class models\n",
    "\n",
    "test_data,test_labels = dp.image_data_and_labels(imagetype, directory, subfolders, class_labels)\n",
    "\n",
    "\n",
    "imagetype = '.jpeg'\n",
    "directory = 'Docs/all_xrays/train/'\n",
    "subfolders = ['normal','virus','bacteria']\n",
    "class_labels = [0,1,1] # Change for binary or multi-class models\n",
    "\n",
    "train_data,train_labels = dp.image_data_and_labels(imagetype, directory, subfolders, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test examples:  (1175, 224, 224, 3)\n",
      "Total number of test labels: (1175, 2)\n",
      "Total number of train examples:  (4625, 224, 224, 3)\n",
      "Total number of train labels: (4625, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of test examples: \", test_data.shape)\n",
    "print(\"Total number of test labels:\", test_labels.shape)\n",
    "print(\"Total number of train examples: \", train_data.shape)\n",
    "print(\"Total number of train labels:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce Model Architecture\n",
    "Due to an error in this version of Keras we cannot directly load the model weights - there is currently no fix but several workarounds. Inconveniently this requires you know the model architecture. From here you can 'train' the model with 0 epochs (to initialise the weights at some value) from there you can load in the weights and test the model. This is what is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = Sequential()\n",
    "\n",
    "model_predict.add(VGG16(include_top = False, input_shape =(224,224,3)).layers[0])\n",
    "model_predict.add(VGG16(include_top = False, input_shape = (224,224,3)).layers[1])\n",
    "model_predict.add(VGG16(include_top = False, input_shape = (224,224,3)).layers[2])\n",
    "model_predict.add(VGG16(include_top = False, input_shape = (224,224,3)).layers[3])\n",
    "\n",
    "model_predict.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_1'))\n",
    "model_predict.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool1'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_1'))\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool2'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_1'))\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool3'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_1'))\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool4'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_1'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_2'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool5'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv6_1'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv6_2'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool6'))\n",
    "\n",
    "model_predict.add(Flatten(name=\"Flatten\"))\n",
    "model_predict.add(Dense(units=1024,activation=\"relu\", name='Dense1'))\n",
    "model_predict.add(Dense(units=512,activation=\"relu\", name='Dense2'))\n",
    "model_predict.add(Dense(units=2, activation=\"softmax\", name='Result'))\n",
    "\n",
    "for layer in model_predict.layers[:3]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train for 0 Epochs just to Initialise Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c37b7d710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "opt = Adam(lr=0.005)\n",
    "checkpoint = ModelCheckpoint(\"baseline.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model_predict.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model_predict.fit(x=train_data,y=train_labels,epochs=0,callbacks=[checkpoint,early],class_weight={0:2.7,1:1}) #Change for binary/multiclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Weights and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[283,  35],\n",
       "       [ 15, 842]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict.load_weights('Best model/Model_4_2class_datagen_OneOf.h5')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predicted labels - argmax will select the largest value (the highest probability) and the corresponding label.\n",
    "preds = model_predict.predict(test_data, batch_size = 16)\n",
    "preds = np.argmax(preds, axis = -1)\n",
    "\n",
    "# Original labels \n",
    "orig_test_labels = np.argmax(test_labels, axis = -1)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(orig_test_labels, preds)\n",
    "cm\n",
    "\n",
    "# Confusion Matrix Layout\n",
    "#\n",
    "#                 True\n",
    "#                +    -\n",
    "# Predicted   +  tp   fp\n",
    "#             -  fn   tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalutation Metrics\n",
    "In this section the model will be evaluated using a **Confusion Matrix**. This can be used to calculate metrics for evaluating classifiers by calculating four values:\n",
    "* **TP** - **True Positives** (Number of Positive cases correctly predicted)\n",
    "* **FP** - **False Positives** (Number of Negative cases predicted as Positive)\n",
    "* **TN** - **True Negatives** (Number of Negative cases correctly predicted)\n",
    "* **FN** - **False Negatives** (Number of Positive cases predicted as Negative)\n",
    "\n",
    "These are then used in the blow metric calculations:\n",
    "\n",
    "* **TPR - True Positive Rate** = $ \\frac{tp}{tp \\; + \\; fn} $\n",
    "\n",
    "* **FPR - False Positive Rate** = $ \\frac{fp}{fp \\; + \\; tn} $\n",
    "\n",
    "* **PPV - Precision** = $ \\frac{tp}{tp \\; + \\; fp} $\n",
    "\n",
    "* **SPC - Specificity** = $ \\frac{tn}{tn \\; + \\; fn} $\n",
    "\n",
    "* **ACC - Accuracy** = $ \\frac{tpr}{tpr \\; + \\; fn} $\n",
    "\n",
    "* **F1** = $ \\frac{tpr \\; * \\; pvv}{tpr \\; + \\; pvv} $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "                True \n",
      "                +   - \n",
      "Predicted   + [283  35]\n",
      "            - [ 15 842]\n",
      "\n",
      "\n",
      "The metrics for this model are:\n",
      "\n",
      "TPR: 94.97% Sick Patients Predicted Sick (Recall)\n",
      "\n",
      "FPR: 3.99% Healthy Patients Predicted Sick (Fall-out)\n",
      "\n",
      "Precision: 88.99% True Sick Patients / Sick Predictions\n",
      "\n",
      "Specificity: 98.25% True Healthy Patients / Healthy Predictions\n",
      "\n",
      "Accuracy: 95.74% Overall Correct Predictions\n",
      "\n",
      "F1: 91.88 \"True\" Accuracy - Our final model evaluation metric\n"
     ]
    }
   ],
   "source": [
    "# Calculate TPR, FPR. Beware of accuracy in imbalanced datasets - we should use ROC curve.\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "tn = cm[1][1]\n",
    "fn = cm[1][0]\n",
    "\n",
    "\n",
    "tpr = round(tp * 100 / (tp + fn), 2)\n",
    "fpr = round(fp * 100 / (fp + tn), 2)\n",
    "ppv = round(tp * 100 / (tp + fp), 2)\n",
    "scc = round(tn * 100 / (tn + fn), 2)\n",
    "acc = round((tp + tn) * 100 / (tp+fp+tn+fn), 2)\n",
    "f1 = round(2 * (tpr * ppv) / (tpr + ppv), 2)\n",
    "\n",
    "print(f'''Confusion Matrix:\\n\\n                True \\n                +   - \\nPredicted   + {cm[0]}\\n            - {cm[1]}\\n\n",
    "\\nThe metrics for this model are:\n",
    "\\nTPR: {tpr}% Sick Patients Predicted Sick (Recall)\n",
    "\\nFPR: {fpr}% Healthy Patients Predicted Sick (Fall-out)\n",
    "\\nPrecision: {ppv}% True Sick Patients / Sick Predictions\n",
    "\\nSpecificity: {scc}% True Healthy Patients / Healthy Predictions\n",
    "\\nAccuracy: {acc}% Overall Correct Predictions\n",
    "\\nF1: {f1} \"True\" Accuracy - Our final model evaluation metric''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
