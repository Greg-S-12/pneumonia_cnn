{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model\n",
    "In this notebook, the **unseen** test data will be used to evaluate the final performance and therefore generalisability of the model.\n",
    "<br>A confusion matrix will be generated to show which samples were correctly/incorrectly classified and the overall accuracy and recall of the model.\n",
    "<br><br> It is important to note the metric used to evaluate the final performance of the model. For an imbalanced dataset accuracy can be misleading. For example:\n",
    "<br> If given a credit card dataset of 99% genuine transactions with only 1% fraudulent. If we wish to classify fraudulent cases, we may find our model has a 99% accuracy but (without generating confusion matrix to see) this could mean we correctly identify the genuine cases (99%) but miss all fraudulent cases (1%) resulting a redundant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import data_prep as dp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "from keras.layers import SeparableConv2D, BatchNormalization, Dropout\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment seed for Python\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "seed=101\n",
    "\n",
    "# Set seed for Numpy\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagetype = '.jpeg'\n",
    "directory = 'Docs/all_xrays/test/'\n",
    "subfolders = ['normal','virus','bacteria']\n",
    "class_labels = [0,1,1] # Change for binary or multi-class models\n",
    "\n",
    "test_data,test_labels = dp.image_data_and_labels(imagetype, directory, subfolders, class_labels)\n",
    "\n",
    "\n",
    "imagetype = '.jpeg'\n",
    "directory = 'Docs/all_xrays/train/'\n",
    "subfolders = ['normal','virus','bacteria']\n",
    "class_labels = [0,1,1] # Change for binary or multi-class models\n",
    "\n",
    "train_data,train_labels = dp.image_data_and_labels(imagetype, directory, subfolders, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of test examples:  (1175, 224, 224, 3)\n",
      "Total number of test labels: (1175, 2)\n",
      "Total number of train examples:  (4625, 224, 224, 3)\n",
      "Total number of train labels: (4625, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of test examples: \", test_data.shape)\n",
    "print(\"Total number of test labels:\", test_labels.shape)\n",
    "print(\"Total number of train examples: \", train_data.shape)\n",
    "print(\"Total number of train labels:\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce Model Architecture\n",
    "Due to an error in this version of Keras we cannot directly load the model weights - there is currently no fix but several workarounds. Inconveniently this requires you know the model architecture. From here you can 'train' the model with 0 epochs (to initialise the weights at some value) from there you can load in the weights and test the model. This is what is done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = Sequential()\n",
    "\n",
    "model_predict.add(VGG16(include_top = False, input_shape =(224,224,3)).layers[0])\n",
    "model_predict.add(VGG16(include_top = False, input_shape = (224,224,3)).layers[1])\n",
    "model_predict.add(VGG16(include_top = False, input_shape = (224,224,3)).layers[2])\n",
    "model_predict.add(VGG16(include_top = False, input_shape = (224,224,3)).layers[3])\n",
    "\n",
    "model_predict.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_1'))\n",
    "model_predict.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool1'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_1'))\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool2'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_1'))\n",
    "model_predict.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool3'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_1'))\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_2'))\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool4'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_1'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_2'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool5'))\n",
    "\n",
    "model_predict.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv6_1'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv6_2'))\n",
    "model_predict.add(BatchNormalization())\n",
    "model_predict.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool6'))\n",
    "\n",
    "model_predict.add(Flatten(name=\"Flatten\"))\n",
    "model_predict.add(Dense(units=1024,activation=\"relu\", name='Dense1'))\n",
    "model_predict.add(Dense(units=512,activation=\"relu\", name='Dense2'))\n",
    "model_predict.add(Dense(units=2, activation=\"softmax\", name='Result'))\n",
    "\n",
    "for layer in model_predict.layers[:3]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train for 0 Epochs just to Initialise Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c37b7d710>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "opt = Adam(lr=0.005)\n",
    "checkpoint = ModelCheckpoint(\"baseline.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "model_predict.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "model_predict.fit(x=train_data,y=train_labels,epochs=0,callbacks=[checkpoint,early],class_weight={0:2.7,1:1}) #Change for binary/multiclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Weights and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[283,  35],\n",
       "       [ 15, 842]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict.load_weights('Best model/Model_4_2class_datagen_OneOf.h5')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Predicted labels - argmax will select the largest value (the highest probability) and the corresponding label.\n",
    "preds = model_predict.predict(test_data, batch_size = 16)\n",
    "preds = np.argmax(preds, axis = -1)\n",
    "\n",
    "# Original labels \n",
    "orig_test_labels = np.argmax(test_labels, axis = -1)\n",
    "\n",
    "# Generate a confusion matrix\n",
    "cm = confusion_matrix(orig_test_labels, preds)\n",
    "cm\n",
    "\n",
    "# SKLearn Confusion Matrix Layout\n",
    "#\n",
    "#                 True\n",
    "#                +    -\n",
    "# Predicted   +  tp   fp\n",
    "#             -  fn   tn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at Recall, TPR, FPR and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.97% of sick patients correctly diagnosed (Recall) \n",
      "98.25% of healthy patients predicted as healthy \n",
      "1.75% of sick patients predicted as healthy \n",
      "3.99% Healthy patients predicted as sick \n",
      "95.74% Overall Correct Predictions \n"
     ]
    }
   ],
   "source": [
    "# Calculate TPR, FPR. Beware of accuracy in imbalanced datasets - we should use ROC curve.\n",
    "\n",
    "tpr = round(cm[0][0]/(cm[1][0]+cm[0][0])*100,2)\n",
    "fnr = round(cm[1][0]/(cm[1][0]+cm[1][1])*100,2)\n",
    "fpr = round(cm[0][1]/(cm[0][1]+cm[1][1])*100,2)\n",
    "tnr = round(cm[1][1]/(cm[1][1]+cm[1][0])*100,2)\n",
    "acc = round((cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,2)\n",
    "\n",
    "print(f'{tpr}% of sick patients correctly diagnosed (Recall) \\n{tnr}% of healthy patients predicted as healthy \\n{fnr}% of sick patients predicted as healthy \\n{fpr}% Healthy patients predicted as sick \\n{acc}% Overall Correct Predictions ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
