{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Organising Data](#first-bullet)\n",
    "* [Formatting the Data](#second-bullet)\n",
    "* [Class Imbalance](#third-bullet)\n",
    "\n",
    "# Preparing the Data\n",
    "This notebook will allow the user to read in the data from Kaggle and prepare it for training and testing. With a few minor changes to code, the user can change: the train/test/validation split sizes, the classes to be predicted (e.g. bacterial/viral/healthy or pneumonia/healthy), the shape of the images for analysis and more.\n",
    "\n",
    "## [Organising the Data](#first-bullet)\n",
    "\n",
    "### Creating Three Classes from Two\n",
    "This dataset is from kaggle and can be found here: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
    "<br> The original dataset has already split the data into train, test and validation sets before splitting them into cases of Pneumonia and Normal (healthy) lungs. In reality Pneumonia can be caused by a bacterial infection or a viral one.\n",
    "<br> For this project, I wish to see if the model can solve a multi-class problem as well as a binary one. Using Convolutional Neural Networks to classify either cases of Pneumonia vs. healthy lungs, before further breaking the pneumonia class down into viral and bacterial. To do so we must create our own classes which requires splitting the images up differently to how this dataset is provided - such that there are 3 different classes: Normal (healthy), Bacterial and Viral.\n",
    "\n",
    "###  Train, Test and Validation Split and Class Imbalance\n",
    "I will further create a train, test and validation split based around these classes. The test set will remained untouched and unseen until the final model is tested  in the third notebook. Validation is used during training to aid the user in avoiding overfitting of the training data.\n",
    "<br> We will also see there is a class imbalance in the dataset which will bias our algorithm in its classifcation. This also makes the accuracy metric (used to evaluate how \"good\" the model is at classifying images) misleading and not a valid representation of the true model performance.\n",
    "<br>\n",
    "## [Formatting the Data](#second-bullet)\n",
    "\n",
    "### Resizing Images\n",
    "As the dataset is a few thousand images all with large resolution we will have to resize these such that the training of our model doesnt take too long. This will be something to consider in optimization of the model too - if transfer . learning is used (such as from VGG16) then the image will have to be of a certain shape and color scheme (grayscale or RGB).\n",
    "\n",
    "### Rescaling\n",
    "This is also known as normalization. Each pixel in an image has a value of 0-255 depending on its contrast. Each image will contain a varying number of each value, meaning a different range of values -  a larger range will contribute more to the loss whereas a smaller range has a smaller contribution. When this is put into our model the network the weights update accordingly. We want each image treated equally so by scaling, we put all the values in the same range 0-1.\n",
    "\n",
    "## [Class Imbalance](#third-bullet)\n",
    "### Image Generation\n",
    "To account for the inital class imbalance we can create new images using the data in our posession. We can do this by flipping, stretching and rotating the images - as long as the augmentation matches what could happen with an actual x-ray image.\n",
    "<br> It is important to note that in this step we will NOT be generating new data IN ADDITION to our training data, but rather we REPLACE the training data in each batch with the augmented images. This is known as \"on-the-fly\" image generation and is the most commonplace method.\n",
    "\n",
    "### Class Weighting\n",
    "An alternative method to data generation is weighting the classes. This will bias the learning so that the undersampled classes are more heavily weighted and therefore all classes are treated equally. This restores the validity of the accuracy metric for model evaluation.\n",
    "<br> While this method has the advantages of being quicker to execute and simpler so less can go wrong, we lose out on training the model on a larger dataset so are more prone to overfitting or worse model performance (on the validation/test sets).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organising Data <a class=\"anchor\" id=\"first-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Set Seed for Reproducibilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import data_prep as dp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment seed for Python\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "seed=101\n",
    "\n",
    "# Set seed for Numpy, TensorFlow and for image augmentation sequence\n",
    "np.random.seed(seed)\n",
    "# aug.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Data Properly\n",
    "We will create a new directory for all images and store them within train, test and validation folders with each class as a subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new directory for our images\n",
    "os.mkdir('all_xrays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'xrays_kaggle/test/PNEUMONIA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-99743bc8d0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0morig_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'xrays_kaggle/{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pneumonia_cnn/data_prep.py\u001b[0m in \u001b[0;36mcopy_files\u001b[0;34m(orig_dir, dest_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m      \u001b[0mdest_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mdirecory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"        \n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'xrays_kaggle/test/PNEUMONIA'"
     ]
    }
   ],
   "source": [
    "# Sets and Classes as on Kaggle\n",
    "# sets = ['test', 'train', 'val']\n",
    "# classes = ['PNEUMONIA','NORMAL']\n",
    "# dest_dir = 'all_xrays'\n",
    "\n",
    "# # Adding the Images to a new directory for Class and Train Splitting\n",
    "# for s in sets:\n",
    "#     for c in classes:\n",
    "#         orig_dir = 'xrays_kaggle/{}/{}'.format(s,c)\n",
    "#         dp.copy_files(orig_dir, dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the folders for each class in each split\n",
    "location = 'all_xrays'\n",
    "splits = ['{}/train'.format(location),'{}/test'.format(location),'{}/val'.format(location)]\n",
    "list_of_classes = ['normal','virus','bacteria']\n",
    "dp.make_classes(splits, list_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of filenames\n",
    "location = '../'\n",
    "normal = dp.find_files(location, \"0001\")\n",
    "virus = dp.find_files(location, \"virus\")\n",
    "bacteria = dp.find_files(location, \"bacteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1582, 1493, 2780)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal),len(virus),len(bacteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform our Train, Test, Validation split and move files to respective folders\n",
    "\n",
    "split_percentages = [0.79, 0.2, 0.01] # Train, Test, Validation\n",
    "files_dict = {'normal':normal,'virus':virus,'bacteria':bacteria}\n",
    "groups = list_of_classes\n",
    "directory = '../'\n",
    "destination_folder = 'all_xrays'\n",
    "\n",
    "dp.move_files_to_groups_by_split(split_percentages, files_dict, groups, directory, destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Class Imbalance\n",
    "We will label each image the corresponding class, create a dataframe and visualize the imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the Data <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "### Importing Libraries to be sed in Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import cv2\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    2196\n",
       "0    1249\n",
       "1    1179\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and label each image according to class\n",
    "filer=dp.Files()\n",
    "\n",
    "normal = filer.find_files('Docs/all_xrays/train/normal', \"0001\").files\n",
    "virus = filer.find_files('Docs/all_xrays/train/virus', \"virus\").files\n",
    "bacteria = filer.find_files('Docs/all_xrays/train/bacteria', \"bacteria\").files\n",
    "\n",
    "files_dict = {'normal':normal,'virus':virus,'bacteria':bacteria}\n",
    "training_dataset = []\n",
    "iteration=-1\n",
    "\n",
    "for files in files_dict:\n",
    "    iteration+=1\n",
    "    for file in files_dict[files]:\n",
    "        training_dataset.append((file,iteration))\n",
    "        \n",
    "df_train = pd.DataFrame(training_dataset, columns=(\"file\",\"class\"))\n",
    "df_train['class'] = df_train['class'].astype(dtype='category')\n",
    "\n",
    "df_train['class'].value_counts() # 0 = Bacterial ; 1 = Viral ; 2 = Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    27\n",
       "0    15\n",
       "1    14\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and label each image according to class\n",
    "filer=dp.Files()\n",
    "\n",
    "normal = filer.find_files('Docs/all_xrays/val/normal', \"0001\").files\n",
    "virus = filer.find_files('Docs/all_xrays/val/virus', \"virus\").files\n",
    "bacteria = filer.find_files('Docs/all_xrays/val/bacteria', \"bacteria\").files\n",
    "\n",
    "files_dict = {'normal':normal,'virus':virus,'bacteria':bacteria}\n",
    "validation_dataset = []\n",
    "iteration=-1\n",
    "\n",
    "for files in files_dict:\n",
    "    iteration+=1\n",
    "    for file in files_dict[files]:\n",
    "        validation_dataset.append((file,iteration))\n",
    "        \n",
    "df_val = pd.DataFrame(validation_dataset, columns=(\"file\",\"class\"))\n",
    "df_val['class'] = df_val['class'].astype(dtype='category')\n",
    "\n",
    "df_val['class'].value_counts() # 2 = Bacterial ; 1 = Viral ; 0 = Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing, Normalizing and Converting to Grayscale\n",
    "Below, I identify the files for each class in either the training or validation samples. I then resize to lower resolution to reduce memory, rescale to normalize the pixels in each image and convert to grayscale as these are x-rayds.\n",
    "<br> To confirm the images are actually in grayscale you can look at the matrix of the pixels in the ikage. You can see for each colour (BGR, as is read by OpenCV) that the values are the same, meaning we can safely convert to grayscale, saving memory and simplifying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e4ff5e601041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Get the list of all the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnormal_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_xrays/val/normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvirus_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_xrays/val/virus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"virus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbacteria_cases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_xrays/val/bacteria'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bacteria\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dp' is not defined"
     ]
    }
   ],
   "source": [
    "val_directory = 'Docs/all_xrays/val/'\n",
    "filer=dp.Files()\n",
    "\n",
    "normal_cases_dir = '{}normal'.format(val_directory)\n",
    "bacteria_cases_dir = '{}bacteria'.format(val_directory)\n",
    "virus_cases_dir = '{}virus'.format(val_directory)\n",
    "\n",
    "\n",
    "# Get the list of all the images\n",
    "normal_cases = filer.find_files('Docs/all_xrays/val/normal', \"0001\").files\n",
    "virus_cases = filer.find_files('Docs/all_xrays/val/virus', \"virus\").files\n",
    "bacteria_cases = filer.find_files('Docs/all_xrays/val/bacteria', \"bacteria\").files\n",
    "\n",
    "\n",
    "# List that are going to contain validation image data and the corresponding labels\n",
    "val_files = []\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "\n",
    "# Normal \n",
    "iteration=0\n",
    "for img in normal_cases:\n",
    "    val_files.append(img)\n",
    "    img = cv2.imread(os.path.join(normal_cases_dir,img)) # Locate our image\n",
    "    img = cv2.resize(img, (224,224))                     # Resize to reduce size of file (to speed up training later). Keeping our ratio of vertical:horizontal widths.\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)          # Convert to grayscale as some files stored as rgb, despite being taken in grayscale. Reduces size as above.\n",
    "    img = img.astype(np.float32)/255.                    # Convert pixels to float and rescale to normalize data (allows traing to converge) and also treats all images equally\n",
    "                                                         # imgaug => forbidden dtypes: (uint32, uint64, uint128, uint256, int32, int64, int128, int256, float64, float96, float128, float256).\n",
    "    label = to_categorical(0, num_classes=3)             # Labels class, number of classes - one-hot encoding\n",
    "    val_data.append(img)\n",
    "    val_labels.append(label)\n",
    "\n",
    "# Virus \n",
    "for img in virus_cases:\n",
    "    val_files.append(img)\n",
    "    img = cv2.imread(os.path.join(virus_cases_dir,img))\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = to_categorical(1, num_classes=3)\n",
    "    val_data.append(img)\n",
    "    val_labels.append(label)\n",
    "    \n",
    "# Bacteria \n",
    "for img in bacteria_cases:\n",
    "    val_files.append(img)\n",
    "    img = cv2.imread(os.path.join(bacteria_cases_dir,img))\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = to_categorical(2, num_classes=3)\n",
    "    val_data.append(img)\n",
    "    val_labels.append(label) \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# Convert the list into numpy arrays\n",
    "val_data = np.array(val_data)\n",
    "val_labels = np.array(val_labels).astype(int)\n",
    "\n",
    "print(\"Total number of validation examples: \", val_data.shape)\n",
    "print(\"Total number of labels:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "### Creating the Sequence\n",
    "Below we will create the sequence to augment our data \"on-the-fly\" to be used in the training of our model.\n",
    "<br> OneOf allows us to choose one of the below augmentations and apply it to each image in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'iaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3755b6dfaee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m seq = iaa.OneOf([iaa.Fliplr(0.5),     # Horizontally flip 50% of the images\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0miaa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAffine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Size of rotation range in degrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0miaa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Makes pixels darker or brighter, random amount between 1.2 and 1.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0miaa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      ]) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'iaa' is not defined"
     ]
    }
   ],
   "source": [
    "seq = iaa.OneOf([iaa.Fliplr(0.5),     # Horizontally flip 50% of the images\n",
    "                    iaa.Affine(rotate=(-20,20)), # Size of rotation range in degrees\n",
    "                    iaa.Multiply((1.2, 1.5)), # Makes pixels darker or brighter, random amount between 1.2 and 1.5\n",
    "                    iaa.Crop(percent=(0, 0.05))\n",
    "                     ]) \n",
    "\n",
    "# There is more one can do here - contrast, rotation etc. - but this is enough to demonstrate its purpose.\n",
    "# It is better to start simple and slowly build up both the data generator and model to avoid overcomplicating things.\n",
    "# This allows the user to more easily identify what changes produce which results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Data Generator \n",
    "The data generator will be used for training of the model. This will allow us to perform augmentations of the original training data in batches and train upon each one of these.  This is particularly important as it will be used to account for the class imbalance by generating more of the undersampled class for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data, batch_size, directory):\n",
    "    \n",
    "    n = len(data)\n",
    "    steps = n//batch_size\n",
    "    \n",
    "    batch_images = np.zeros((batch_size, 224,224,3),dtype=np.float32)\n",
    "    batch_labels = np.zeros((batch_size, 3),dtype=np.float32)\n",
    "    \n",
    "    indices = np.arange(n)\n",
    "    \n",
    "    # Initialize a counter\n",
    "    i = 0\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        # Get the next batch \n",
    "        count = 0\n",
    "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
    "        for j, idx in enumerate(next_batch):\n",
    "            img_name = data.iloc[idx]['file']\n",
    "            label = data.iloc[idx]['class']            \n",
    "            \n",
    "            # read the image and resize\n",
    "            if \"0001\" in img_name:\n",
    "                img = cv2.imread(str(os.path.join(\"{}normal\".format(directory),str(img_name))))\n",
    "                img = cv2.resize(img, (224,224)) # Setting the size of all the images as 224x224 - standard input size for VGG-16\n",
    "            \n",
    "            elif \"virus\" in img_name:\n",
    "                img = cv2.imread(str(os.path.join(\"{}virus\".format(directory),str(img_name))))\n",
    "                img = cv2.resize(img, (224,224))\n",
    "                                 \n",
    "            else:\n",
    "                img = cv2.imread(str(os.path.join(\"{}bacteria\".format(directory),str(img_name))))\n",
    "                img = cv2.resize(img, (224,224))\n",
    "            \n",
    "            # one hot encoding\n",
    "            encoded_label = to_categorical(label, num_classes=3)\n",
    "            \n",
    "            if img.shape[2]==1:       \n",
    "                img = np.dstack([img, img, img])  # If grayscale then converts to rgb.\n",
    "            \n",
    "            # cv2 reads in BGR mode by default\n",
    "            orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # normalize the image pixels\n",
    "            orig_img = img.astype(np.float32)/255.\n",
    "            \n",
    "            batch_images[count] = orig_img\n",
    "            batch_labels[count] = encoded_label\n",
    "            \n",
    "            # generating more samples of the undersampled class\n",
    "            if label==0 and count < batch_size-2:\n",
    "                aug_img_n = seq.augment_image(img)\n",
    "                aug_img_n = cv2.cvtColor(aug_img_n, cv2.COLOR_BGR2RGB)\n",
    "                aug_img_n = aug_img_n.astype(np.float32)/255.\n",
    "\n",
    "                batch_images[count+1] = aug_img_n\n",
    "                batch_labels[count+1] = encoded_label\n",
    "                \n",
    "                count +=1\n",
    "\n",
    "            elif label==1 and count < batch_size-2:\n",
    "                aug_img_v = seq.augment_image(img)\n",
    "                aug_img_v = cv2.cvtColor(aug_img_v, cv2.COLOR_BGR2RGB)\n",
    "                aug_img_v = aug_img_v.astype(np.float32)/255.\n",
    "                \n",
    "                batch_images[count+2] = aug_img_v\n",
    "                batch_labels[count+2] = encoded_label\n",
    "\n",
    "                count +=1\n",
    "            \n",
    "            else:\n",
    "                count+=1\n",
    "            \n",
    "            if count==batch_size-1:\n",
    "                break\n",
    "            \n",
    "        i+=1\n",
    "        yield batch_images, batch_labels\n",
    "            \n",
    "        if i>=steps:\n",
    "            i=0    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting the classes\n",
    "If the user wishes to avoid using data generation - class weightings can be used. The weightings are input during  training of the model. Below, the weightings are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1249, 2196, 1179)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find files for each class in training dataset\n",
    "filer=dp.Files()\n",
    "\n",
    "# Create list of files\n",
    "normal = filer.find_files('Docs/all_xrays/train/normal', \"0001\").files\n",
    "virus = filer.find_files('Docs/all_xrays/train/virus', \"virus\").files\n",
    "bacteria = filer.find_files('Docs/all_xrays/train/bacteria', \"bacteria\").files\n",
    "\n",
    "#Sample populations\n",
    "len(normal),len(bacteria),len(virus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.86, 1.76, 2.7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine weightings\n",
    "\n",
    "# 3 Class problem\n",
    "virus_weighting = len(bacteria)/len(virus)\n",
    "normal_weighting = len(bacteria)/len(normal)\n",
    "\n",
    "# Binary Problem\n",
    "normal_weighting_binary = (len(virus)+len(bacteria))/len(normal)\n",
    "\n",
    "# Print weights (to 2dp)\n",
    "round(virus_weighting,2), round(normal_weighting,2), round(normal_weighting_binary,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "I will be using the VGG16 neural network as this has been pre-trained on ImageNet (dataset of over 14 million images), achieving 92.7% accuracy. It surpasses AlexNet network by replacing large filters of size 11 and 5 in the first and second convolution layers with small size 3x3 filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model\n",
    "This willbe a baseline model as it has a simple structure with no dropout, batch normalization or separable convolutional. It conists of 2D convolutions and a max pooling layer as well as initial transfer earning from VGG16.\n",
    "\n",
    "I will also specify the learning rate of the optimiser, here in this case it is set at 0.001. If our training loss begins to explode, we will have likely overshot the global minimum and as a result should reduce the learning rate. If we find the loss is not decreasing substantially over several epochs it could be a sign to increase the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "vggmodel = VGG16()\n",
    "# vggmodel.summary()\n",
    "# vggmodel.save('vgg16_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "# VGG16 first few layers - currently not \"frozen\".\n",
    "model.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[0])\n",
    "model.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[1])\n",
    "model.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[2])\n",
    "model.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[3])\n",
    "\n",
    "# First 2D convolutions (x3) then max pooling. Produces 256 filters.\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_1'))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_2'))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv1_3'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool1'))\n",
    "\n",
    "# Second 2D convolutions (x3) then max pooling. Produces 512 filters.\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_1'))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_2'))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_3'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool2'))\n",
    "\n",
    "# Thirds 2D convolutions (x3) then max pooling. Produces 512 filters.\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_1'))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_2'))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_3'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool3'))\n",
    "\n",
    "# Flatten all weights into one layer - this produces the largest number of parameters\n",
    "# Further reduce the units (number of neurons) before using a final softmax layer to produce our 3 outputs (classes)\n",
    "model.add(Flatten(name=\"Flatten\"))\n",
    "model.add(Dense(units=1024, activation=\"relu\", name='Dense1'))\n",
    "model.add(Dense(units=512, activation=\"relu\", name='Dense2'))\n",
    "model.add(Dense(units=3, activation=\"softmax\", name='Result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "Conv3_1 (Conv2D)             (None, 112, 112, 256)     147712    \n",
      "_________________________________________________________________\n",
      "Conv3_2 (Conv2D)             (None, 112, 112, 256)     590080    \n",
      "_________________________________________________________________\n",
      "Conv3_3 (Conv2D)             (None, 112, 112, 256)     590080    \n",
      "_________________________________________________________________\n",
      "Pool3 (MaxPooling2D)         (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "Conv4_1 (Conv2D)             (None, 56, 56, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "Conv4_2 (Conv2D)             (None, 56, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Conv4_3 (Conv2D)             (None, 56, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Pool4 (MaxPooling2D)         (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "Conv5_1 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Conv5_2 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Conv5_3 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Pool5 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 1024)              102761472 \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "Result (Dense)               (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 117,633,603\n",
      "Trainable params: 117,594,883\n",
      "Non-trainable params: 38,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Freezing the initial layers (VGG16) to prevent training and allow transfer learning to our model.\n",
    "\n",
    "for layer in model.layers[:3]:\n",
    "    layer.trainable=False\n",
    "\n",
    "# See the full model architecture, image shape + number of filters and the number of parameters in each layer.\n",
    "# Fewer parameters reduces training time.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If in local minima while training, Adam optimiser will help get out of local minima and reach global minimum.\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Initial learning rate - affects the rate of gradient descent. Increase if loss does not reduce.\n",
    "opt = Adam(lr=0.001) \n",
    "# Compiles model for training. As we are looking at categorical classes we use categorical cross-entropy loss function.\n",
    "# Accuracy will be evaluated and displayed at the end of each epoch.\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"baseline.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.7191 - accuracy: 0.7120 - val_loss: 0.7432 - val_accuracy: 0.7480\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 35s 3s/step - loss: 0.7726 - accuracy: 0.6540 - val_loss: 0.7080 - val_accuracy: 0.6880\n",
      "Epoch 3/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.6660 - accuracy: 0.7260 - val_loss: 0.7783 - val_accuracy: 0.6020\n",
      "Epoch 4/40\n",
      "10/10 [==============================] - 35s 3s/step - loss: 0.7213 - accuracy: 0.7100 - val_loss: 0.6028 - val_accuracy: 0.7080\n",
      "Epoch 5/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.5961 - accuracy: 0.7580 - val_loss: 0.6060 - val_accuracy: 0.7640\n",
      "Epoch 6/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.5589 - accuracy: 0.7720 - val_loss: 0.5316 - val_accuracy: 0.7340\n",
      "Epoch 7/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.5282 - accuracy: 0.7720 - val_loss: 0.6226 - val_accuracy: 0.7300\n",
      "Epoch 8/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.5993 - accuracy: 0.7600 - val_loss: 0.8876 - val_accuracy: 0.7540\n",
      "Epoch 9/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.6225 - accuracy: 0.7380 - val_loss: 0.7423 - val_accuracy: 0.7040\n",
      "Epoch 10/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.6076 - accuracy: 0.7440 - val_loss: 0.5598 - val_accuracy: 0.7440\n",
      "Epoch 11/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.5822 - accuracy: 0.7560 - val_loss: 0.5669 - val_accuracy: 0.7640\n",
      "Epoch 12/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4940 - accuracy: 0.7860 - val_loss: 0.5977 - val_accuracy: 0.8180\n",
      "Epoch 13/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.5760 - accuracy: 0.7840 - val_loss: 0.6067 - val_accuracy: 0.8100\n",
      "Epoch 14/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4537 - accuracy: 0.8120 - val_loss: 0.6463 - val_accuracy: 0.7920\n",
      "Epoch 15/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.5471 - accuracy: 0.7620 - val_loss: 0.5781 - val_accuracy: 0.7900\n",
      "Epoch 16/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.5125 - accuracy: 0.7760 - val_loss: 0.4978 - val_accuracy: 0.8100\n",
      "Epoch 17/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.4956 - accuracy: 0.7880 - val_loss: 0.5615 - val_accuracy: 0.7740\n",
      "Epoch 19/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.4613 - accuracy: 0.7800 - val_loss: 0.6027 - val_accuracy: 0.7800\n",
      "Epoch 20/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4847 - accuracy: 0.7780 - val_loss: 0.6950 - val_accuracy: 0.7660\n",
      "Epoch 21/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.4332 - accuracy: 0.8140 - val_loss: 0.5038 - val_accuracy: 0.8020\n",
      "Epoch 22/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4148 - accuracy: 0.8320 - val_loss: 0.6401 - val_accuracy: 0.7520\n",
      "Epoch 23/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.3752 - accuracy: 0.8440 - val_loss: 0.5533 - val_accuracy: 0.7640\n",
      "Epoch 24/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4492 - accuracy: 0.8120 - val_loss: 0.5976 - val_accuracy: 0.7800\n",
      "Epoch 25/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.4282 - accuracy: 0.8180 - val_loss: 0.6105 - val_accuracy: 0.8140\n",
      "Epoch 26/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.4125 - accuracy: 0.8160 - val_loss: 0.5088 - val_accuracy: 0.7880\n",
      "Epoch 27/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.4269 - accuracy: 0.8080 - val_loss: 0.4620 - val_accuracy: 0.8520\n",
      "Epoch 28/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.4092 - accuracy: 0.8180 - val_loss: 0.5748 - val_accuracy: 0.8100\n",
      "Epoch 29/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.3940 - accuracy: 0.8520 - val_loss: 0.5678 - val_accuracy: 0.8100\n",
      "Epoch 30/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4932 - accuracy: 0.7820 - val_loss: 0.5672 - val_accuracy: 0.8160\n",
      "Epoch 31/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4646 - accuracy: 0.8040 - val_loss: 0.5074 - val_accuracy: 0.7680\n",
      "Epoch 32/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.3981 - accuracy: 0.8220 - val_loss: 0.5328 - val_accuracy: 0.7920\n",
      "Epoch 33/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.3966 - accuracy: 0.8500 - val_loss: 0.6020 - val_accuracy: 0.7820\n",
      "Epoch 34/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.4195 - accuracy: 0.8400 - val_loss: 0.6644 - val_accuracy: 0.7580\n",
      "Epoch 35/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.3814 - accuracy: 0.8520 - val_loss: 0.7240 - val_accuracy: 0.8100\n",
      "Epoch 36/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4335 - accuracy: 0.8100 - val_loss: 0.8089 - val_accuracy: 0.7580\n",
      "Epoch 37/40\n",
      "10/10 [==============================] - 33s 3s/step - loss: 0.3963 - accuracy: 0.8440 - val_loss: 0.5596 - val_accuracy: 0.8000\n",
      "Epoch 39/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.3986 - accuracy: 0.8440 - val_loss: 0.4460 - val_accuracy: 0.8160\n",
      "Epoch 40/40\n",
      "10/10 [==============================] - 34s 3s/step - loss: 0.4121 - accuracy: 0.8220 - val_loss: 0.5586 - val_accuracy: 0.7740\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "tf.set_random_seed(101)\n",
    "\n",
    "# Fit model and begin training.\n",
    "hist = model.fit_generator(steps_per_epoch=10,generator=data_generator(df_train,batch_size,'Docs/all_xrays/train/'),\n",
    "                           validation_data=data_generator(df_val,batch_size,'Docs/all_xrays/val/'), \n",
    "                           validation_steps=10,epochs=40,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights of the model as an h5 file. Save only when necessary as files are LARGE! (Github limit=2GB)\n",
    "\n",
    "# model.save('Baseline_Model_10steps.h5')\n",
    "# model = load_model('Baseline_Model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9a2b0225a390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# If val_loss decreases while training increases - OVERFITTING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot the accuracy and loss for the training and validation steps for every epoch.\n",
    "# Reveals how much the model is improving and an idea of overfitting \n",
    "# If val_loss decreases while training increases - OVERFITTING\n",
    "\n",
    "plt.plot(hist.history[\"accuracy\"])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "plt.show()\n",
    "# plt.savefig(\"Baseline_Model_10steps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot we can see the loss for both datasets appears to be decreasing while the accuracy for both are also increasing -  apart from some large spikes. We can therefore conclude that we are not overfitting but may be underfitting and could continue training the model for more epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
