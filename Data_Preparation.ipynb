{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Organising Data](#first-bullet)\n",
    "* [Formatting the Data](#second-bullet)\n",
    "* [Augmenting the Data](#third-bullet)\n",
    "\n",
    "# Preparing the Data\n",
    "\n",
    "## Organising the Data\n",
    "\n",
    "### Creating Three Classes from Two\n",
    "This dataset is from kaggle and can be found here: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
    "<br> The original dataset has already split the data into train, test and validation sets before splitting them into cases of Pneumonia and Normal (healthy) lungs. In reality Pneumonia can be caused by a bacterial infection or a viral one.\n",
    "<br> For this project, I wish to solve a multi-class problem and use Convolutional Neural Networks to classify first cases of Pneumonia vs. Normal, before further breaking this down in viral and bacterial Pneumonia. To do so we must create our own classes which requires splitting the images up Normal, Bacterial and Viral.\n",
    "\n",
    "###  Train, Test and Validation Split and Class Imbalance\n",
    "I will further create a train, test and validation split based around these classes.\n",
    "<br> We will see that in doing so, we will have a class imbalance in our training dataset which will bias our algorithm in its classifcation.\n",
    "<br>\n",
    "## Formatting the Data\n",
    "\n",
    "### Resizing Images\n",
    "As the dataset isa few thousand images allwith large resolution - ~1000 x 600 - we will have to resize these such that the training of our model doesnt take too long. This will be something to consider in optimization of the model too.\n",
    "\n",
    "### Rescaling\n",
    "This is also known as normalization. Each pixel in an image has a value of 0-255 depending on its contrast. Each image will contain a varying number of each value, meaning a different range of values -  a larger range will contribute more to the loss whereas a smaller range has a smaller contribution. When this is put into our model the network the weights update accordingly. We want each image treated equally so by scaling, we put all the values in the same range 0-1.\n",
    "\n",
    "## Augmenting the Data\n",
    "### Image Generation\n",
    "To account for the inital class imbalance we will need to create new images using the data in our posession. We do this by flipping, stretching and rotating the images.\n",
    "<br> It is important to note that in this step we will NOT be generating new data IN ADDITION to our training data, but rather we REPLACE the training data in each batch with the augmented images. This is known as \"on-the-fly\" image generation and is the most commonplace method.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organising Data <a class=\"anchor\" id=\"first-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Set Seed for Reproducibilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import data_prep as dp\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the environment seed for Python\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "seed=101\n",
    "\n",
    "# Set seed for Numpy, TensorFlow and for image augmentation sequence\n",
    "# np.random.seed(seed)\n",
    "# tf.set_random_seed(seed)\n",
    "# aug.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Data Properly\n",
    "We will create a new directory for all images and store them within train, test and validation folders with each class as a subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'all_xrays'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-52366fd9fd67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make new directory for our images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all_xrays'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'all_xrays'"
     ]
    }
   ],
   "source": [
    "# Make new directory for our images\n",
    "os.mkdir('all_xrays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'xrays_kaggle/test/PNEUMONIA'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-99743bc8d0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0morig_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'xrays_kaggle/{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pneumonia_cnn/data_prep.py\u001b[0m in \u001b[0;36mcopy_files\u001b[0;34m(orig_dir, dest_dir)\u001b[0m\n\u001b[1;32m     31\u001b[0m      \u001b[0mdest_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mdirecory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \"\"\"        \n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'xrays_kaggle/test/PNEUMONIA'"
     ]
    }
   ],
   "source": [
    "# Sets and Classes as on Kaggle\n",
    "sets = ['test', 'train', 'val']\n",
    "classes = ['PNEUMONIA','NORMAL']\n",
    "dest_dir = 'all_xrays'\n",
    "\n",
    "# Adding the Images to a new directory for Class and Train Splitting\n",
    "for s in sets:\n",
    "    for c in classes:\n",
    "        orig_dir = 'xrays_kaggle/{}/{}'.format(s,c)\n",
    "        dp.copy_files(orig_dir, dest_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the folders for each class in each split\n",
    "location = 'all_xrays'\n",
    "splits = ['{}/train'.format(location),'{}/test'.format(location),'{}/val'.format(location)]\n",
    "list_of_classes = ['normal','virus','bacteria']\n",
    "dp.make_classes(splits, list_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of filenames\n",
    "location = '../'\n",
    "normal = dp.find_files(location, \"0001\")\n",
    "virus = dp.find_files(location, \"virus\")\n",
    "bacteria = dp.find_files(location, \"bacteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform our Train, Test, Validation split and move files to respective folders\n",
    "\n",
    "split_percentages = [0.79, 0.2, 0.01] # Train, Test, Validation\n",
    "files_dict = {'normal':normal,'virus':virus,'bacteria':bacteria}\n",
    "groups = list_of_classes\n",
    "directory = '../'\n",
    "destination_folder = 'all_xrays'\n",
    "\n",
    "dp.move_files_to_groups_by_split(split_percentages, files_dict, groups, directory, destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Class Imbalance\n",
    "We will label each image the corresponding class, create a dataframe and visualize the imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1727\n",
       "1    1249\n",
       "2     793\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and label each image according to class\n",
    "\n",
    "normal = dp.find_files('all_xrays/train/normal', \"0001\")\n",
    "virus = dp.find_files('all_xrays/train/virus', \"virus\")\n",
    "bacteria = dp.find_files('all_xrays/train/bacteria', \"bacteria\")\n",
    "\n",
    "files_dict = {'normal':normal,'virus':virus,'bacteria':bacteria}\n",
    "training_dataset = []\n",
    "iteration=-1\n",
    "\n",
    "for files in files_dict:\n",
    "    iteration+=1\n",
    "    for file in files_dict[files]:\n",
    "        training_dataset.append((file,iteration))\n",
    "        \n",
    "df_train = pd.DataFrame(training_dataset, columns=(\"file\",\"class\"))\n",
    "df_train['class'] = df_train['class'].astype(dtype='category')\n",
    "\n",
    "df_train['class'].value_counts() # 2 = Bacterial ; 1 = Viral ; 0 = Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person21_bacteria_73.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person142_bacteria_682.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>person1719_bacteria_4541.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>person799_bacteria_2705.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person30_bacteria_148.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            file class\n",
       "0      person21_bacteria_73.jpeg     0\n",
       "1    person142_bacteria_682.jpeg     0\n",
       "2  person1719_bacteria_4541.jpeg     0\n",
       "3   person799_bacteria_2705.jpeg     0\n",
       "4     person30_bacteria_148.jpeg     0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21\n",
       "1    15\n",
       "2    10\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find and label each image according to class\n",
    "\n",
    "normal = dp.find_files('all_xrays/val/normal', \"0001\")\n",
    "virus = dp.find_files('all_xrays/val/virus', \"virus\")\n",
    "bacteria = dp.find_files('all_xrays/val/bacteria', \"bacteria\")\n",
    "\n",
    "files_dict = {'normal':normal,'virus':virus,'bacteria':bacteria}\n",
    "validation_dataset = []\n",
    "iteration=-1\n",
    "\n",
    "for files in files_dict:\n",
    "    iteration+=1\n",
    "    for file in files_dict[files]:\n",
    "        validation_dataset.append((file,iteration))\n",
    "        \n",
    "df_val = pd.DataFrame(validation_dataset, columns=(\"file\",\"class\"))\n",
    "df_val['class'] = df_val['class'].astype(dtype='category')\n",
    "\n",
    "df_val['class'].value_counts() # 2 = Bacterial ; 1 = Viral ; 0 = Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-606081251958>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Class_0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Normal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class_1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Viral'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class_2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Bacterial'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# df = pd.get_dummies(data=df, columns=['Class'])\n",
    "# df = df.rename(columns={'Class_0':'Normal', 'Class_1':'Viral', 'Class_2':'Bacterial'})\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c2c1dd240>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 10, 21)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(normal),len(virus),len(bacteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7582065652522019 1.8625954198473282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2196.0, 2196.0)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number to multiply images of each class to account for classes  imbalance\n",
    "normal_overample_rate = len(bacteria)/len(normal)\n",
    "virus_overample_rate = len(bacteria)/len(virus)\n",
    "print(normal_overample_rate,virus_overample_rate)\n",
    "# Sanity check\n",
    "len(normal)*normal_overample_rate,len(virus)*virus_overample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting the Data <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "### Importing Libraries to be sed in Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'imgaug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0f2983edb870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'imgaug'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import data_prep as dp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_directory = 'all_xrays/train/'\n",
    "# val_directory = 'all_xrays/val/'\n",
    "\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         train_directory,        # This is the target directory\n",
    "#         target_size=(200, 140), # Resize images to 200x140\n",
    "#         batch_size=20,\n",
    "#         classes = None,         # Defaultclass labels from sub-driectory names. Alphanumeric labels(b=0,n=1,v=2)\n",
    "#         class_mode='categorical')\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "#         val_directory,\n",
    "#         target_size=(200, 140),\n",
    "#         batch_size=20,\n",
    "#         classes = None,   \n",
    "#         class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing, Normalizing and Converting to Grayscale\n",
    "Below, I identify the files for each class in either the training or validation samples. I then resize to lower resolution to reduce memory, rescale to normalize the pixels in each image and convert to grayscale as these are x-rayds.\n",
    "<br> To confirm the images are actually in grayscale you can look at the matrix of the pixels in the ikage. You can see for each colour (BGR, as is read by OpenCV) that the values are the same, meaning we can safely convert to grayscale, saving memory and simplifying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ec991e39cc7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormal_cases\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mval_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_cases_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Locate our image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                     \u001b[0;31m# Resize to reduce size of file (to speed up training later). Keeping our ratio of vertical:horizontal widths.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# Convert to grayscale as some files stored as rgb, despite being taken in grayscale. Reduces size as above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "val_directory = 'all_xrays/val/'\n",
    "\n",
    "normal_cases_dir = '{}normal'.format(val_directory)\n",
    "bacteria_cases_dir = '{}bacteria'.format(val_directory)\n",
    "virus_cases_dir = '{}virus'.format(val_directory)\n",
    "\n",
    "\n",
    "# Get the list of all the images\n",
    "normal_cases = dp.find_files('all_xrays/val/normal', \"0001\")\n",
    "virus_cases = dp.find_files('all_xrays/val/virus', \"virus\")\n",
    "bacteria_cases = dp.find_files('all_xrays/val/bacteria', \"bacteria\")\n",
    "\n",
    "\n",
    "# List that are going to contain validation image data and the corresponding labels\n",
    "val_files = []\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "\n",
    "\n",
    "# Normal \n",
    "iteration=0\n",
    "for img in normal_cases:\n",
    "    val_files.append(img)\n",
    "    img = cv2.imread(os.path.join(normal_cases_dir,img)) # Locate our image\n",
    "    img = cv2.resize(img, (140,200))                     # Resize to reduce size of file (to speed up training later). Keeping our ratio of vertical:horizontal widths.\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)          # Convert to grayscale as some files stored as rgb, despite being taken in grayscale. Reduces size as above.\n",
    "    img = img.astype(np.float32)/255.                    # Convert pixels to float and rescale to normalize data (allows traing to converge) and also treats all images equally\n",
    "                                                         # imgaug => forbidden dtypes: (uint32, uint64, uint128, uint256, int32, int64, int128, int256, float64, float96, float128, float256).\n",
    "    label = to_categorical(0, num_classes=3)             # Labels class, number of classes - one-hot encoding\n",
    "    val_data.append(img)\n",
    "    val_labels.append(label)\n",
    "\n",
    "# Virus \n",
    "for img in virus_cases:\n",
    "    val_files.append(img)\n",
    "    img = cv2.imread(os.path.join(virus_cases_dir,img))\n",
    "    img = cv2.resize(img, (140,200))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = to_categorical(1, num_classes=3)\n",
    "    val_data.append(img)\n",
    "    val_labels.append(label)\n",
    "    \n",
    "# Bacteria \n",
    "for img in bacteria_cases:\n",
    "    val_files.append(img)\n",
    "    img = cv2.imread(os.path.join(bacteria_cases_dir,img))\n",
    "    img = cv2.resize(img, (140,200))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype(np.float32)/255.\n",
    "    label = to_categorical(2, num_classes=3)\n",
    "    val_data.append(img)\n",
    "    val_labels.append(label) \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# Convert the list into numpy arrays\n",
    "val_data = np.array(val_data)\n",
    "val_labels = np.array(val_labels).astype(int)\n",
    "\n",
    "print(\"Total number of validation examples: \", val_data.shape)\n",
    "print(\"Total number of labels:\", val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting the Data <a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "### Creating the Sequence\n",
    "Below we will create the sequence to augment our data \"on-the-fly\" to be used in the training of our model.\n",
    "<br> OneOf allows us to choose one of the below augmentations and apply it to each image in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.OneOf([iaa.Fliplr(0.5),     # Horizontally flip 50% of the images\n",
    "                    iaa.Affine(rotate=(-20,20)), # Size of rotation range in degrees\n",
    "                    iaa.Multiply((1.2, 1.5)), # Makes pixels darker or brighter, random amount between 1.2 and 1.5\n",
    "                    iaa.Crop(percent=(0, 0.05))\n",
    "                     ]) \n",
    "\n",
    "# contrast etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_aug = seq(images=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Data Generator \n",
    "The data generator will be used for training of the model. This will allow us to perform augmentations of the original training data in batches and train upon each one of these.  This is particularly important as it will be used to account for the class imbalance by generating more of the undersampled class for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]],\n",
       "\n",
       "       [[0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_aug)\n",
    "resolution=(1,2)\n",
    "resolution[0]\n",
    "batch_size = int(len(val_data)/2)\n",
    "np.zeros((batch_size, resolution[0],resolution[1]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-88c7beb5e82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# df = pd.get_dummies(data=df, columns=['class'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# df = df.rename(columns={'class_0':'normal','class_1':'virus','class_2':'bacteria'})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_train = pd.DataFrame(training_dataset)\n",
    "df_train = df.rename(columns={0:'file',1:'class'})\n",
    "# df = pd.get_dummies(data=df, columns=['class'])\n",
    "# df = df.rename(columns={'class_0':'normal','class_1':'virus','class_2':'bacteria'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_grayscale(data, number_of_batches, resolution):\n",
    "    \n",
    "    n = len(data)\n",
    "    batch_size = int(n/(number_of_batches))\n",
    "    steps = n//batch_size\n",
    "    \n",
    "    batch_images = np.zeros((batch_size, resolution[0],resolution[1]),dtype=np.float32)\n",
    "    batch_labels = np.zeros((batch_size, 3),dtype=np.float32)\n",
    "    \n",
    "    indices = np.arange(n)\n",
    "    \n",
    "    # Initialize a counter\n",
    "    i = 0\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        # Get the next batch \n",
    "        count = 0\n",
    "        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n",
    "        for j, idx in enumerate(next_batch):\n",
    "            img_name = data.iloc[idx]['file']\n",
    "            label = data.iloc[idx]['class']            \n",
    "            \n",
    "            \n",
    "            # one hot encoding\n",
    "            encoded_label = to_categorical(label, num_classes=3)\n",
    "              \n",
    "            # read the image and resize\n",
    "            img = cv2.imread(str(img_name))\n",
    "            img = cv2.resize(img, (224,224)) # Setting the size of all the images as 224x224 - standard input size for VGG-16\n",
    "\n",
    "            \n",
    "            # cv2 reads in BGR mode by default\n",
    "            orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            # normalize the image pixels\n",
    "            orig_img = img.astype(np.float32)/255.\n",
    "            \n",
    "            batch_data[count] = orig_img\n",
    "            batch_labels[count] = encoded_label\n",
    "            \n",
    "            # generating more samples of the undersampled class\n",
    "            if label==0 and count < batch_size-2:\n",
    "                aug_img_n = seq.augment_image(img)\n",
    "                aug_img_n = cv2.cvtColor(aug_img_n, cv2.COLOR_BGR2GRAY)\n",
    "                aug_img_n = aug_img_n.astype(np.float32)/255.\n",
    "\n",
    "                batch_data[count+1] = aug_img_n\n",
    "                batch_labels[count+1] = encoded_label\n",
    "                \n",
    "                count +=1\n",
    "\n",
    "            elif label==1 and count < batch_size-2:\n",
    "                aug_img_v = seq.augment_image(img)\n",
    "                aug_img_v = cv2.cvtColor(aug_img_v, cv2.COLOR_BGR2GRAY)\n",
    "                aug_img_v = aug_img_v.astype(np.float32)/255.\n",
    "                \n",
    "                batch_data[count+2] = aug_img_v\n",
    "                batch_labels[count+2] = encoded_label\n",
    "\n",
    "                count +=1\n",
    "            \n",
    "            else:\n",
    "                count+=1\n",
    "            \n",
    "            if count==batch_size-1:\n",
    "                break\n",
    "            \n",
    "        i+=1\n",
    "        yield batch_data, batch_labels\n",
    "            \n",
    "        if i>=steps:\n",
    "            i=0    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object data_generator_grayscale at 0x1a19000888>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test\n",
    "data_generator_grayscale(df_train,10,(224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "I will be using the VGG16 neural network as this has been pre-trained on ImageNet (dataset of over 14 million images), achieving 92.7% accuracy. It surpasses AlexNet network by replacing large filters of size 11 and 5 in the first and second convolution layers with small size 3x3 filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model\n",
    "This is the baseline model as it is the most basic. It conists of 2d convolutions and a pooling layer.\n",
    "\n",
    "I We will also specify the learning rate of the optimiser, here in this case it is set at 0.001. If our training is bouncing a lot on epochs then we need to decrease the learning rate so that we can reach global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "vggmodel = VGG16()\n",
    "# vggmodel.summary()\n",
    "# vggmodel.save('vgg16_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", name='Conv1_1'))\n",
    "# model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", name='Conv1_2'))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool1'))\n",
    "\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_1'))\n",
    "# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv2_2'))\n",
    "# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool2'))\n",
    "\n",
    "model.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[0])\n",
    "model.add(VGG16(include_top=False, input_shape=(224,224,3)).layers[1])\n",
    "                        \n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_1'))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_2'))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv3_3'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool3'))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_1'))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_2'))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv4_3'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool4'))\n",
    "\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_1'))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_2'))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name='Conv5_3'))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), name='Pool5'))\n",
    "\n",
    "model.add(Flatten(name=\"Flatten\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\", name='Dense1'))\n",
    "model.add(Dense(units=4096,activation=\"relu\", name='Dense2'))\n",
    "model.add(Dense(units=3, activation=\"softmax\", name='Result'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "Conv3_1 (Conv2D)             (None, 224, 224, 256)     147712    \n",
      "_________________________________________________________________\n",
      "Conv3_2 (Conv2D)             (None, 224, 224, 256)     590080    \n",
      "_________________________________________________________________\n",
      "Conv3_3 (Conv2D)             (None, 224, 224, 256)     590080    \n",
      "_________________________________________________________________\n",
      "Pool3 (MaxPooling2D)         (None, 112, 112, 256)     0         \n",
      "_________________________________________________________________\n",
      "Conv4_1 (Conv2D)             (None, 112, 112, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "Conv4_2 (Conv2D)             (None, 112, 112, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "Conv4_3 (Conv2D)             (None, 112, 112, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "Pool4 (MaxPooling2D)         (None, 56, 56, 512)       0         \n",
      "_________________________________________________________________\n",
      "Conv5_1 (Conv2D)             (None, 56, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Conv5_2 (Conv2D)             (None, 56, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Conv5_3 (Conv2D)             (None, 56, 56, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "Pool5 (MaxPooling2D)         (None, 28, 28, 512)       0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 401408)            0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 4096)              1644171264\n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "Result (Dense)               (None, 3)                 12291     \n",
      "=================================================================\n",
      "Total params: 1,675,273,731\n",
      "Trainable params: 1,675,273,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If in local minima while training, Adam optimiser will help get out of local minima and reach global minima.\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3123\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3124\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3125\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-919be44526b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mearly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m hist = model.fit_generator(steps_per_epoch=100,generator=data_generator_grayscale(df_train,10,(224,224)),\n\u001b[0;32m----> 5\u001b[0;31m                            validation_data= data_generator_grayscale(df_val,10,(224,224)), validation_steps=10,epochs=100,callbacks=[checkpoint,early])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m                 \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_data_generator_task\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    656\u001b[0m                             \u001b[0;31m# => Serialize calls to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                             \u001b[0;31m# infinite iterator/generator's next() function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                             \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-220-a15747052fc6>\u001b[0m in \u001b[0;36mdata_generator_grayscale\u001b[0;34m(data, number_of_batches, resolution)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnext_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3130\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3132\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3133\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3134\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"baseline.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=100,generator=data_generator_grayscale(df_train,10,(224,224)),\n",
    "                           validation_data= data_generator_grayscale(df_val,10,(224,224)), validation_steps=10,epochs=100,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Model\n",
    "In this model I will take advantage of depthwise convolution and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
